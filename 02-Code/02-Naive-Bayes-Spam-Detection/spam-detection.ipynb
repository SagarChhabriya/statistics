{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection Using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = [\n",
    "    \"To use your credit, click the new WAP link in the next years txt message or click here\", \n",
    "    \"Thanks for your subscription to New Ringtone UK your new mobile will be charged £5/month Please confirm annoncement by replying\", \n",
    "    \"As a valued customer, I am pleased to advise you that following recent delivery waiting review of your Mob No. you are awarded with. Call us to review.\", \n",
    "    \"Please call our new customer service representative on\", \n",
    "    \"We are trying to contact you. Last weekends customer draw shows that you won a £1000 prize GUARANTEED. Calling years\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave one sentence from spam for testing our model later \n",
    "spam_test = [\"Customer service annoncement. You have a New Years delivery waiting for you. click\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "non = [\n",
    "    \"I don't think he goes to usf, he lives around here though\", \n",
    "    \"New car and house for my parents. i have only new job in hand\", \n",
    "    \"Great escape. I fancy the bridge but needs her lager. See you tomorrow\", \n",
    "    \"Tired. I haven't slept well the past few nights.\",\n",
    "    \"Too late. I said i have the website. I didn't i have or dont have the slippers\", \n",
    "    \"I might come by tonight then if my class lets out early\", \n",
    "    \"Jos ask if u wana meet up?\", \n",
    "    \"That would be great. We'll be at the Guild. We can try meeting with the customer on Bristol road or somewhere\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another sentence from non for testing our model \n",
    "spam_test_2 = [\"That would be great. We'll be at the Guild. We can try meeting with the customer on Bristol road or somewhere\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.1\n",
      "    Uninstalling numpy-2.1.1:\n",
      "      Successfully uninstalled numpy-2.1.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.0.5 wrapt-1.17.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = non[0]\n",
    "test_sentence = non[5]\n",
    "# test_sentence = spam[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I might come by tonight then if my class lets out early\n",
      "I come tonight class lets early\n",
      "i come tonight class lets earli\n",
      "['i', 'come', 'tonight', 'class', 'lets', 'earli']\n"
     ]
    }
   ],
   "source": [
    "print(test_sentence)\n",
    "\n",
    "# 1. Remove stop words: \n",
    "removed_stops = remove_stopwords(test_sentence)\n",
    "print(removed_stops)\n",
    "\n",
    "# 2. Stemming\n",
    "p = PorterStemmer()\n",
    "stemmed = p.stem(removed_stops)\n",
    "print(stemmed)\n",
    "\n",
    "# 3. Tokenization\n",
    "tokens = tokenize(stemmed)\n",
    "print(list(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stop Words\n",
    "Stop words are common words in a language (like \"and,\" \"the,\" \"in,\" \"is\") that are often ignored in tasks like text processing or search engines because they don't carry significant meaning.\n",
    "\n",
    "**Example:**  \n",
    "In the sentence \"The cat is on the mat,\" the stop words are \"the\" and \"is.\"\n",
    "\n",
    "### 2. Stemming\n",
    "Stemming is the process of reducing words to their base or root form.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "stemmed_word = p.stem(\"running\")  # Output: \"run\"\n",
    "```\n",
    "### 3. Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into individual units, such as words or phrases, called **tokens**. These tokens are the building blocks for further text processing tasks.\n",
    "\n",
    "**Example:**\n",
    "The sentence \"running on the mat\" after stemming becomes \"run on mat\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    removed_stops = remove_stopwords(sentence)\n",
    "    p = PorterStemmer()\n",
    "    stemmed = p.stem(removed_stops)\n",
    "    tokens = tokenize(stemmed)\n",
    "\n",
    "    return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = set() # will have unique values only\n",
    "spams_tokenized = []\n",
    "nons_tokenized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized spam:  [['to', 'use', 'credit', 'click', 'new', 'wap', 'link', 'years', 'txt', 'message', 'click'], ['to', 'use', 'credit', 'click', 'new', 'wap', 'link', 'years', 'txt', 'message', 'click'], ['to', 'use', 'credit', 'click', 'new', 'wap', 'link', 'years', 'txt', 'message', 'click'], ['thanks', 'subscription', 'new', 'ringtone', 'uk', 'new', 'mobile', 'charged', 'month', 'please', 'confirm', 'annoncement', 'repli'], ['as', 'valued', 'customer', 'i', 'pleased', 'advise', 'following', 'recent', 'delivery', 'waiting', 'review', 'mob', 'no', 'awarded', 'with', 'call', 'review'], ['please', 'new', 'customer', 'service', 'repres'], ['we', 'trying', 'contact', 'you', 'last', 'weekends', 'customer', 'draw', 'shows', 'won', 'prize', 'guaranteed', 'calling', 'year'], ['to', 'use', 'credit', 'click', 'new', 'wap', 'link', 'years', 'txt', 'message', 'click'], ['to', 'use', 'credit', 'click', 'new', 'wap', 'link', 'years', 'txt', 'message', 'click'], ['to', 'use', 'credit', 'click', 'new', 'wap', 'link', 'years', 'txt', 'message', 'click'], 'To use your credit, click the new WAP link in the next years txt message or click here', 'Thanks for your subscription to New Ringtone UK your new mobile will be charged £5/month Please confirm annoncement by replying', 'As a valued customer, I am pleased to advise you that following recent delivery waiting review of your Mob No. you are awarded with. Call us to review.', 'Please call our new customer service representative on', 'We are trying to contact you. Last weekends customer draw shows that you won a £1000 prize GUARANTEED. Calling years', 'To use your credit, click the new WAP link in the next years txt message or click here', ['to', 'use', 'credit', 'click', 'new', 'wap', 'link', 'years', 'txt', 'message', 'click'], ['thanks', 'subscription', 'new', 'ringtone', 'uk', 'new', 'mobile', 'charged', 'month', 'please', 'confirm', 'annoncement', 'repli'], ['as', 'valued', 'customer', 'i', 'pleased', 'advise', 'following', 'recent', 'delivery', 'waiting', 'review', 'mob', 'no', 'awarded', 'with', 'call', 'review'], ['please', 'new', 'customer', 'service', 'repres'], ['we', 'trying', 'contact', 'you', 'last', 'weekends', 'customer', 'draw', 'shows', 'won', 'prize', 'guaranteed', 'calling', 'year'], ['to', 'use', 'credit', 'click', 'new', 'wap', 'link', 'years', 'txt', 'message', 'click'], ['thanks', 'subscription', 'new', 'ringtone', 'uk', 'new', 'mobile', 'charged', 'month', 'please', 'confirm', 'annoncement', 'repli'], ['as', 'valued', 'customer', 'i', 'pleased', 'advise', 'following', 'recent', 'delivery', 'waiting', 'review', 'mob', 'no', 'awarded', 'with', 'call', 'review'], ['please', 'new', 'customer', 'service', 'repres'], ['we', 'trying', 'contact', 'you', 'last', 'weekends', 'customer', 'draw', 'shows', 'won', 'prize', 'guaranteed', 'calling', 'year']]\n",
      "Tokenized non:  [['i', 'don', 't', 'think', 'goes', 'usf', 'l'], ['i', 'don', 't', 'think', 'goes', 'usf', 'l'], ['new', 'car', 'house', 'parents', 'new', 'job', 'hand'], ['great', 'escape', 'i', 'fancy', 'bridge', 'needs', 'lager', 'see', 'tomorrow'], ['tired', 'i', 'haven', 't', 'slept', 'past', 'nights'], ['too', 'late', 'i', 'said', 'website', 'i', 'didn', 't', 'dont', 'slipp'], ['i', 'come', 'tonight', 'class', 'lets', 'earli'], ['jos', 'ask', 'u', 'wana', 'meet', 'up'], ['that', 'great', 'we', 'll', 'guild', 'we', 'try', 'meeting', 'customer', 'bristol', 'road'], ['i', 'don', 't', 'think', 'goes', 'usf', 'l'], ['new', 'car', 'house', 'parents', 'new', 'job', 'hand'], ['great', 'escape', 'i', 'fancy', 'bridge', 'needs', 'lager', 'see', 'tomorrow'], ['tired', 'i', 'haven', 't', 'slept', 'past', 'nights'], ['too', 'late', 'i', 'said', 'website', 'i', 'didn', 't', 'dont', 'slipp'], ['i', 'come', 'tonight', 'class', 'lets', 'earli'], ['jos', 'ask', 'u', 'wana', 'meet', 'up'], ['that', 'great', 'we', 'll', 'guild', 'we', 'try', 'meeting', 'customer', 'bristol', 'road']]\n",
      "dictionary:  {'usf', 'l', 'tired', 'subscription', 'year', 'delivery', 'house', 'slipp', 'u', 'month', 'didn', 'use', 'guild', 'years', 'road', 'needs', 'service', 'won', 'message', 'calling', 'don', 'repres', 'mob', 'goes', 'come', 'haven', 'credit', 'advise', 'try', 'no', 'you', 'ringtone', 'we', 'ask', 'pleased', 'too', 'meet', 'see', 'tomorrow', 'said', 'jos', 'car', 'lets', 'slept', 'lager', 'i', 'fancy', 'bridge', 'draw', 'contact', 'that', 'tonight', 'thanks', 'customer', 'review', 'nights', 'with', 'earli', 'job', 'click', 'link', 'think', 'hand', 'dont', 'up', 'uk', 'mobile', 'guaranteed', 'trying', 'confirm', 'wana', 'wap', 'meeting', 'prize', 'll', 't', 'class', 'weekends', 'annoncement', 'shows', 'following', 'escape', 'parents', 'website', 'new', 'late', 'txt', 'recent', 'to', 'repli', 'waiting', 'awarded', 'charged', 'valued', 'as', 'call', 'bristol', 'please', 'last', 'great', 'past'}\n"
     ]
    }
   ],
   "source": [
    "for sentence in spam:\n",
    "    sentence_tokens = tokenize_sentence(sentence)\n",
    "    spams_tokenized.append(sentence_tokens)\n",
    "    dictionary = dictionary.union(sentence_tokens)\n",
    "\n",
    "\n",
    "for sentence in non:\n",
    "    sentence_tokens = tokenize_sentence(sentence)\n",
    "    nons_tokenized.append(sentence_tokens)\n",
    "    dictionary = dictionary.union(sentence_tokens)\n",
    "\n",
    "\n",
    "print(\"Tokenized spam: \",spams_tokenized) # spams_tokenized hold spam tokens\n",
    "print(\"Tokenized non: \", nons_tokenized) # nons_tokenized hold non spam tokens\n",
    "print(\"dictionary: \", dictionary) # dictionary holds all tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of words:  101\n"
     ]
    }
   ],
   "source": [
    "# These things do not depend on an individual word so let's calculate them separately once \n",
    "\n",
    "total_word_count = len(dictionary) # total words\n",
    "total_spam_messages = len(spams_tokenized)\n",
    "total_all_messages = len(spams_tokenized) + len(nons_tokenized)\n",
    "\n",
    "print(\"Total Number of words: \", total_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam) =  0.6046511627906976\n"
     ]
    }
   ],
   "source": [
    "# P(spam) ... does not depend on an individual word so let's calculate that separately once \n",
    "\n",
    "p_spam = total_spam_messages / total_all_messages\n",
    "\n",
    "print(\"P(spam) = \", p_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count occurences\n",
    "\n",
    "def count_word_in_messages(word, messages):\n",
    "    total_count = 0\n",
    "    for msg in messages: \n",
    "        if word in msg:\n",
    "            total_count += 1\n",
    "    return total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Actual Probability Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prob = 1 # can't start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'great', 'we', 'll', 'guild', 'we', 'try', 'meeting', 'customer', 'bristol', 'road']\n",
      "----------------\n",
      "Running for word: that\n",
      "P( w | spam)  =  0.07692307692307693\n",
      "P( w )        =  0.09302325581395349\n",
      "P( spam )     =  0.6046511627906976\n",
      "P( spam | w ) =  0.5\n",
      "\n",
      "----------------\n",
      "Running for word: great\n",
      "P( w | spam)  =  0.0\n",
      "P( w )        =  0.09302325581395349\n",
      "P( spam )     =  0.6046511627906976\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "----------------\n",
      "Running for word: we\n",
      "P( w | spam)  =  0.15384615384615385\n",
      "P( w )        =  0.13953488372093023\n",
      "P( spam )     =  0.6046511627906976\n",
      "P( spam | w ) =  0.6666666666666666\n",
      "\n",
      "----------------\n",
      "Running for word: ll\n",
      "P( w | spam)  =  0.15384615384615385\n",
      "P( w )        =  0.13953488372093023\n",
      "P( spam )     =  0.6046511627906976\n",
      "P( spam | w ) =  0.6666666666666666\n",
      "\n",
      "----------------\n",
      "Running for word: guild\n",
      "P( w | spam)  =  0.0\n",
      "P( w )        =  0.046511627906976744\n",
      "P( spam )     =  0.6046511627906976\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "----------------\n",
      "Running for word: we\n",
      "P( w | spam)  =  0.15384615384615385\n",
      "P( w )        =  0.13953488372093023\n",
      "P( spam )     =  0.6046511627906976\n",
      "P( spam | w ) =  0.6666666666666666\n",
      "\n",
      "----------------\n",
      "Running for word: try\n",
      "P( w | spam)  =  0.038461538461538464\n",
      "P( w )        =  0.06976744186046512\n",
      "P( spam )     =  0.6046511627906976\n",
      "P( spam | w ) =  0.3333333333333333\n",
      "\n",
      "----------------\n",
      "Running for word: meeting\n",
      "P( w | spam)  =  0.0\n",
      "P( w )        =  0.046511627906976744\n",
      "P( spam )     =  0.6046511627906976\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "----------------\n",
      "Running for word: customer\n",
      "P( w | spam)  =  0.46153846153846156\n",
      "P( w )        =  0.32558139534883723\n",
      "P( spam )     =  0.6046511627906976\n",
      "P( spam | w ) =  0.8571428571428571\n",
      "\n",
      "----------------\n",
      "Running for word: bristol\n",
      "P( w | spam)  =  0.0\n",
      "P( w )        =  0.046511627906976744\n",
      "P( spam )     =  0.6046511627906976\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "----------------\n",
      "Running for word: road\n",
      "P( w | spam)  =  0.0\n",
      "P( w )        =  0.046511627906976744\n",
      "P( spam )     =  0.6046511627906976\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "P( spam | all_words ) =  0.0\n"
     ]
    }
   ],
   "source": [
    "for test_sentence in spam_test_2: \n",
    "    test_sentence = tokenize_sentence(test_sentence)\n",
    "    print(test_sentence)\n",
    "    \n",
    "    # let's run this for each word separately \n",
    "    for word in test_sentence: \n",
    "        print(\"----------------\")\n",
    "        print(\"Running for word:\", word)\n",
    "        \n",
    "        # Find P( w | spam)\n",
    "        spam_count = count_word_in_messages(word, spams_tokenized)\n",
    "        p_w_spam = spam_count / total_spam_messages \n",
    "        print(\"P( w | spam)  = \", p_w_spam)\n",
    "        \n",
    "        # Find P( w )\n",
    "        w_count = count_word_in_messages(word, spams_tokenized)\n",
    "        w_count += count_word_in_messages(word, nons_tokenized)\n",
    "        p_w = w_count / total_all_messages\n",
    "        print(\"P( w )        = \", p_w)\n",
    "        \n",
    "        \n",
    "        # Find P( spam | w )\n",
    "        p_spam_w = (p_w_spam * p_spam) / p_w\n",
    "        print(\"P( spam )     = \", p_spam)\n",
    "        print(\"P( spam | w ) = \", p_spam_w)\n",
    "        print(\"\")\n",
    "        final_prob *= p_spam_w\n",
    "        \n",
    "        \n",
    "    print(\"P( spam | all_words ) = \", final_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
